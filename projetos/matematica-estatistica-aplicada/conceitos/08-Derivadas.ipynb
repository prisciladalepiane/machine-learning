{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66ffd99",
   "metadata": {},
   "source": [
    "# Derivada \n",
    "## Conceito de derivada\n",
    "\n",
    "Matemáticamente, a derivada em um ponto é a inclinação da linha tangente à curva da função naquele ponto. A linha tangente é a linha q toca a curva em apenas um ponto e tem a mesma direção que a curva nesse ponto.\n",
    "\n",
    "Dizemos que a derivada é a taxa de variação de uma função $y=f(x)$ em relação à x, dada pela relação:\n",
    "\n",
    "$$\\delta x/ \\delta y$$\n",
    "\n",
    "Considerando uma função $y = f(x)$, a sua derivada no ponto $x = x_0$ corresponde à tangente do ângulo formado pela interseção entre a reta e a curva da função $y = f(x)$, isto é, o coeficiente angular da reta tangente à curva.\n",
    "\n",
    "A derivada é indicada como $f'(x)$ e pode ser calculada através do limite.\n",
    "\n",
    "**Def 1.** Seja $f$ uma função e $p$ um ponto do seu domínio. O limite:\n",
    "\n",
    "$$\\lim_{x \\to p} \\frac{f(x) - f(x)}{x - p}$$\n",
    "quando existe e é finito, o chamamos de derivada de $f$ em $p$ e indica-se por $f'(p)$. Ou seja:\n",
    "$$f'(p)  \\lim_{x \\to p} \\frac{f(x) - f(x)}{x - p}$$\n",
    "Se $f$ admite derivada mem $p$, então dizemos que $f$ é diferenciável ou derivável em $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72a0c1",
   "metadata": {},
   "source": [
    "## Derivada em python\n",
    "\n",
    "A derivada mede a sensibilidade à mudança da função (valor de saída) em relação a uma mudança na sua entrada.\n",
    "\n",
    "Em termos mais simples, a derivada de uma função em um ponto específico é a taxa na qual a função está mudando naquele ponto. Isso é frequentemente entendido como a inclinação da linha tangente à função naquele ponto.\n",
    "\n",
    "Por exemplo, se temos uma função que descreve a posição de um carro em movimento ao longo do tempo, a derivada dessa função em um ponto específico nos dá a velocidade de carro naquele momento.\n",
    "\n",
    "A derivada de uma função f(x) é normalmente escrita como f'(x) ou df/dx. O processo de encontrar a derivada é chamado de diferenciação.\n",
    "\n",
    "A derivada de $$f(x) = x^n$$ é dada por $$f'(x) = n x^{(n-1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fb8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sympy\n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbce50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo variável simbolica\n",
    "x = symbols('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48925a57",
   "metadata": {},
   "source": [
    "f'(x) = x³ 2x² - x + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67df0aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3 x^{2} + 4 x - 1$"
      ],
      "text/plain": [
       "3*x**2 + 4*x - 1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao = x**3 + 2*x**2 - x + 1\n",
    "derivada = diff(funcao, x)\n",
    "derivada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60724",
   "metadata": {},
   "source": [
    "no ponto x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4fef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6$"
      ],
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada.subs(x, 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906ffad",
   "metadata": {},
   "source": [
    "Derivada da função f(x) = 2x²:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341f0fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = x**2\n",
    "diff(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe30a1",
   "metadata": {},
   "source": [
    "## Representação geométrica\n",
    "\n",
    " 1. Escolher a função:\n",
    "        f(x) = x²\n",
    " 2. Plotar a função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "# Gerando valores de x\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculando f(x) e f'(x)\n",
    "y = f(x)\n",
    "dy = df(x)\n",
    "\n",
    "# Plotando a função e sua derivada\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, label='f(x) = x²', color='blue')\n",
    "plt.plot(x, dy, label=\"f'(x) = 2x\", color='red', linestyle='--')\n",
    "plt.title('Função e sua Derivada')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axhline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.axvline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54668bca",
   "metadata": {},
   "source": [
    "## Função Composta - REgra da Cadeia (Chain Rule)\n",
    "\n",
    "A Regra da Cadeia (Chain Rule) é uma fórmula para calcular a derivada de uma composição de funções.\n",
    "É usada quando temos uma \"função dentro de uma função\", também conhecida como função composta.\n",
    "\n",
    "Vamos considerar duas funções, $f(x)$ e $g(x)$. Se temos uma função $h(x)$ que é a composição dessas duas funções, isto é, $h(x) = f(g(x))$, então a Regra da Cadeia diz que a derivada de $h(x)$ é a derivada de f em relação a g, multiplicada pela derivada de g em relação a x.\n",
    "\n",
    "Matematicamente, isso é expresso da seguinte maneira:\n",
    "\n",
    "$$h'(x) = f'(g(x)) * g'(x)$$\n",
    "\n",
    "Essa fórmula nos diz que, para derivar a função composta h(x) = f(g(x)), primeiro derivamos a função externa f em relação à função interna g, e então multiplicamos pelo resultado da derivação da função interna g em relação a x.\n",
    "\n",
    "Vamos ilustrar isso com um exemplo:\n",
    "\n",
    "Suponha que temos\n",
    " $$h(x) = (3x + 1)^2$$\n",
    "Esta é uma composição de \n",
    " $$f(u) = u^2 \\\\ g(x) = 3x + 1$$\n",
    " \n",
    "\n",
    " Se quisermos encontrar h'(x), \n",
    " primeiro derivamos f(u) em relação a u para obter 2u, e então substituímos u por g(x) para obter 2*(3x + 1). Depois derivamos g(x) em relação a x para obter 3. \n",
    "\n",
    "$$f'(u)=2u \\\\ g'(x) = 3$$\n",
    "\n",
    "Finalmente, multiplicamos esses dois resultados para obter \n",
    "$$h'(x) = f'(u).g'(x) =\\\\ 2 . (3x + 1) . 3 = \\\\ 6 . (3x + 1) = \\\\ 18x + 6 $$\n",
    "\n",
    "\n",
    "https://www.deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce89664",
   "metadata": {},
   "source": [
    "### Regra da Cadeia em python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d03e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b96d3",
   "metadata": {},
   "source": [
    "\n",
    "Podemos usar a biblioteca `sympy` para calcular a derivada de uma função composta. \n",
    "\n",
    "Vamos considerar a função $h(x) = (3x + 1)^2$ como mencionado na explicação acima.\n",
    "\n",
    "Aqui está o código Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5bcc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3064bc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 18 x + 6$"
      ],
      "text/plain": [
       "18*x + 6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo a função composta h(x) = (3x + 1)^2\n",
    "h = (3*x + 1)**2\n",
    "\n",
    "# Calculando a derivada de h(x) usando a regra da cadeia\n",
    "derivada_h = diff(h, x)\n",
    "derivada_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51f2c1",
   "metadata": {},
   "source": [
    "## Aplicando regra da cadeira em redes neurais artificiais\n",
    "\n",
    "O uso mais comum da regra da cadeia em redes neurais artificiais está na implementação do algoritmo de retropropagação (backpropagation), que é usado para treinar redes neurais.\n",
    "\n",
    "A retropropagação é um algoritmo que calcula o gradiente da função de perda (loss function) com respeito aos pesos da rede. Esse gradiente é então usado para ajustar os pesos na direção que minimiza a perda. A regra da cadeia é usada para calcular esse gradiente.\n",
    "\n",
    "O gradiente de uma função é um vetor que contém as derivadas parciais da função em relação a cada uma de suas variáveis. Ele fornece a direção do maior aumento da função e a magnitude desse aumento é dada pelo valor do gradiente naquele ponto.\n",
    "\n",
    "A regra da cadeia é um teorema no cálculo que permite a diferenciação de funções compostas. No contexto de funções de múltiplas variáveis, a regra da cadeia permite calcular a derivada de uma função composta considerando as derivadas das funções componentes.\n",
    "\n",
    "Quando se calcula o gradiente de uma função composta usando a regra da cadeia, o que se obtém é uma expressão para a taxa de variação da função composta em relação a cada uma de suas variáveis. Em outras palavras, o gradiente resultante nos dá a direção e magnitude do maior aumento da função composta no espaço de várias dimensões.\n",
    "\n",
    "Essa informação é extremamente útil em uma série de aplicações, incluindo otimização de funções, onde se deseja encontrar o ponto mínimo ou máximo de uma função, bem como em métodos numéricos e aplicações de Machine Learning, como no treinamento de redes neurais com o método do gradiente descendente.\n",
    "\n",
    "Vamos implementar uma rede neural simples com apenas um neurônio (também chamado de perceptron) para demonstrar isso. Usaremos a biblioteca PyTorch, que lida automaticamente com a regra da cadeia durante a retropropagação.\n",
    "\n",
    "Considere Fórmula Matemática: \n",
    "$$y = x * w$$ \n",
    "\n",
    "**y**: variável de saída\\\n",
    "**x**: variável de entrada\\\n",
    "**w**: o que estabelece esse relacionamento de x e y\n",
    "\n",
    "Para criar nosso exemplo, vamos definir os conceitos abaixo:\n",
    "\n",
    "**Cálculo do Gradiente**: Em redes neurais, o processo de aprendizado envolve otimizar os parâmetros (ou pesos) da rede para minimizar a função de perda (ou erro). Isso é feito usando técnicas de otimização como o gradiente descendente. Para aplicar essas técnicas, é necessário calcular o gradiente da função de perda em relação a cada parâmetro. O gradiente é essencialmente a taxa de mudança da função de perda com respeito a esses parâmetros, ou seja, a derivada.\n",
    "\n",
    "**Backpropagation**: O cálculo do gradiente é realizado através de um processo chamado backpropagation. Para isso, as bibliotecas de aprendizado de máquina mantêm um grafo de computação que registra todas as operações realizadas nos tensores que têm requires_grad definido como True. Quando a função de perda é calculada, o gradiente dessa perda é propagado de volta através do grafo e os gradientes em relação a cada tensor são acumulados.\n",
    "\n",
    "**requires_grad=True**: Ao definir requires_grad=True para um tensor no PyTorch, você está informando à biblioteca que deseja que ela calcule os gradientes desse tensor durante a passagem para trás (backpropagation). Normalmente, isso é feito para os parâmetros da rede que você deseja otimizar. Por exemplo, pesos e vieses em uma rede neural teriam requires_grad=True.\n",
    "\n",
    "**Otimização e Atualização de Parâmetros**: Durante o treinamento, esses gradientes são usados por otimizadores (como SGD, Adam, etc.) para atualizar os parâmetros da rede na direção que minimiza a função de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6467bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43243c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de entrada x\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9000b145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300, requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de peso w (coeficiente do modelo, o que o modelo vai aprender no treinamento )\n",
    "w = torch.tensor(0.03, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e912153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de saída y (previsão do modelo)\n",
    "y = torch.tensor(1.0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e82208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função de ativação como função de identidade (f(x) = x)\n",
    "funcao_ativacao = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c55c7b",
   "metadata": {},
   "source": [
    "Uma das principais razões para usar funções de ativação é introduzir não linearidade no modelo. Redes neurais são projetadas para aproximar funções complexas e a maioria dos problemas do mundo real que queremos modelar são não lineares por natureza. Sem funções de ativação não lineares, uma rede neural, independentemente de sua profundidade (número de camadas), seria equivalente a um modelo linear e, portanto, incapaz de modelar a complexidade encontrada em tarefas reais como reconhecimento de imagem, processamento de linguagem natural, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec70728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula a saída da rede neural\n",
    "y_previsto = funcao_ativacao(w * x)\n",
    "y_previsto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e21df",
   "metadata": {},
   "source": [
    "Calcular o erro quadrado médio (MSE, do inglês Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cb27dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9409, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao_de_erro = (y_previsto - y)**2\n",
    "funcao_de_erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fcfe8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.7600)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao_de_erro = torch.nn.MSELoss()\n",
    "erro = funcao_de_erro(y_previsto, y)\n",
    "# Usa a retropropagação para calcular o gradiente do erro em relação a w\n",
    "erro.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2511f",
   "metadata": {},
   "source": [
    "O script Python acima define um neurônio com uma entrada (x) e um peso (w), e usa a regra da cadeia para calcular o gradiente da função de perda com respeito ao peso. O valor de gradiente resultante pode ser usado para atualizar o peso e treinar a rede neural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66ffd99",
   "metadata": {},
   "source": [
    "# Derivada \n",
    "## Conceito de derivada\n",
    "\n",
    "Matemáticamente, a derivada em um ponto é a inclinação da linha tangente à curva da função naquele ponto. A linha tangente é a linha q toca a curva em apenas um ponto e tem a mesma direção que a curva nesse ponto.\n",
    "\n",
    "Dizemos que a derivada é a taxa de variação de uma função $y=f(x)$ em relação à x, dada pela relação:\n",
    "\n",
    "$$\\delta x/ \\delta y$$\n",
    "\n",
    "Considerando uma função $y = f(x)$, a sua derivada no ponto $x = x_0$ corresponde à tangente do ângulo formado pela interseção entre a reta e a curva da função $y = f(x)$, isto é, o coeficiente angular da reta tangente à curva.\n",
    "\n",
    "A derivada é indicada como $f'(x)$ e pode ser calculada através do limite.\n",
    "\n",
    "**Def 1.** Seja $f$ uma função e $p$ um ponto do seu domínio. O limite:\n",
    "\n",
    "$$\\lim_{x \\to p} \\frac{f(x) - f(x)}{x - p}$$\n",
    "quando existe e é finito, o chamamos de derivada de $f$ em $p$ e indica-se por $f'(p)$. Ou seja:\n",
    "$$f'(p)  \\lim_{x \\to p} \\frac{f(x) - f(x)}{x - p}$$\n",
    "Se $f$ admite derivada mem $p$, então dizemos que $f$ é diferenciável ou derivável em $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72a0c1",
   "metadata": {},
   "source": [
    "## Derivada em python\n",
    "\n",
    "A derivada mede a sensibilidade à mudança da função (valor de saída) em relação a uma mudança na sua entrada.\n",
    "\n",
    "Em termos mais simples, a derivada de uma função em um ponto específico é a taxa na qual a função está mudando naquele ponto. Isso é frequentemente entendido como a inclinação da linha tangente à função naquele ponto.\n",
    "\n",
    "Por exemplo, se temos uma função que descreve a posição de um carro em movimento ao longo do tempo, a derivada dessa função em um ponto específico nos dá a velocidade de carro naquele momento.\n",
    "\n",
    "A derivada de uma função f(x) é normalmente escrita como f'(x) ou df/dx. O processo de encontrar a derivada é chamado de diferenciação.\n",
    "\n",
    "A derivada de $$f(x) = x^n$$ é dada por $$f'(x) = n x^{(n-1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fb8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sympy\n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbce50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo variável simbolica\n",
    "x = symbols('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48925a57",
   "metadata": {},
   "source": [
    "f'(x) = x³ 2x² - x + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67df0aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3 x^{2} + 4 x - 1$"
      ],
      "text/plain": [
       "3*x**2 + 4*x - 1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao = x**3 + 2*x**2 - x + 1\n",
    "derivada = diff(funcao, x)\n",
    "derivada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60724",
   "metadata": {},
   "source": [
    "no ponto x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd4fef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6$"
      ],
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivada.subs(x, 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906ffad",
   "metadata": {},
   "source": [
    "Derivada da função f(x) = 2x²:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341f0fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x$"
      ],
      "text/plain": [
       "2*x"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = x**2\n",
    "diff(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe30a1",
   "metadata": {},
   "source": [
    "## Representação geométrica\n",
    "\n",
    " 1. Escolher a função:\n",
    "        f(x) = x²\n",
    " 2. Plotar a função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "# Gerando valores de x\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculando f(x) e f'(x)\n",
    "y = f(x)\n",
    "dy = df(x)\n",
    "\n",
    "# Plotando a função e sua derivada\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, y, label='f(x) = x²', color='blue')\n",
    "plt.plot(x, dy, label=\"f'(x) = 2x\", color='red', linestyle='--')\n",
    "plt.title('Função e sua Derivada')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axhline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.axvline(0, color='black',linewidth=0.5, ls='--')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54668bca",
   "metadata": {},
   "source": [
    "## Função Composta - REgra da Cadeia (Chain Rule)\n",
    "\n",
    "A Regra da Cadeia (Chain Rule) é uma fórmula para calcular a derivada de uma composição de funções.\n",
    "É usada quando temos uma \"função dentro de uma função\", também conhecida como função composta.\n",
    "\n",
    "Vamos considerar duas funções, $f(x)$ e $g(x)$. Se temos uma função $h(x)$ que é a composição dessas duas funções, isto é, $h(x) = f(g(x))$, então a Regra da Cadeia diz que a derivada de $h(x)$ é a derivada de f em relação a g, multiplicada pela derivada de g em relação a x.\n",
    "\n",
    "Matematicamente, isso é expresso da seguinte maneira:\n",
    "\n",
    "$$h'(x) = f'(g(x)) * g'(x)$$\n",
    "\n",
    "Essa fórmula nos diz que, para derivar a função composta h(x) = f(g(x)), primeiro derivamos a função externa f em relação à função interna g, e então multiplicamos pelo resultado da derivação da função interna g em relação a x.\n",
    "\n",
    "Vamos ilustrar isso com um exemplo:\n",
    "\n",
    "Suponha que temos\n",
    " $$h(x) = (3x + 1)^2$$\n",
    "Esta é uma composição de \n",
    " $$f(u) = u^2 \\\\ g(x) = 3x + 1$$\n",
    " \n",
    "\n",
    " Se quisermos encontrar h'(x), \n",
    " primeiro derivamos f(u) em relação a u para obter 2u, e então substituímos u por g(x) para obter 2*(3x + 1). Depois derivamos g(x) em relação a x para obter 3. \n",
    "\n",
    "$$f'(u)=2u \\\\ g'(x) = 3$$\n",
    "\n",
    "Finalmente, multiplicamos esses dois resultados para obter \n",
    "$$h'(x) = f'(u).g'(x) =\\\\ 2 . (3x + 1) . 3 = \\\\ 6 . (3x + 1) = \\\\ 18x + 6 $$\n",
    "\n",
    "\n",
    "https://www.deeplearningbook.com.br/algoritmo-backpropagation-parte1-grafos-computacionais-e-chain-rule/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce89664",
   "metadata": {},
   "source": [
    "### Regra da Cadeia em python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d03e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import symbols, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b96d3",
   "metadata": {},
   "source": [
    "\n",
    "Podemos usar a biblioteca `sympy` para calcular a derivada de uma função composta. \n",
    "\n",
    "Vamos considerar a função $h(x) = (3x + 1)^2$ como mencionado na explicação acima.\n",
    "\n",
    "Aqui está o código Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5bcc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3064bc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 18 x + 6$"
      ],
      "text/plain": [
       "18*x + 6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo a função composta h(x) = (3x + 1)^2\n",
    "h = (3*x + 1)**2\n",
    "\n",
    "# Calculando a derivada de h(x) usando a regra da cadeia\n",
    "derivada_h = diff(h, x)\n",
    "derivada_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51f2c1",
   "metadata": {},
   "source": [
    "## Aplicando regra da cadeira em redes neurais artificiais\n",
    "\n",
    "O uso mais comum da regra da cadeia em redes neurais artificiais está na implementação do algoritmo de retropropagação (backpropagation), que é usado para treinar redes neurais.\n",
    "\n",
    "A retropropagação é um algoritmo que calcula o gradiente da função de perda (loss function) com respeito aos pesos da rede. Esse gradiente é então usado para ajustar os pesos na direção que minimiza a perda. A regra da cadeia é usada para calcular esse gradiente.\n",
    "\n",
    "O gradiente de uma função é um vetor que contém as derivadas parciais da função em relação a cada uma de suas variáveis. Ele fornece a direção do maior aumento da função e a magnitude desse aumento é dada pelo valor do gradiente naquele ponto.\n",
    "\n",
    "A regra da cadeia é um teorema no cálculo que permite a diferenciação de funções compostas. No contexto de funções de múltiplas variáveis, a regra da cadeia permite calcular a derivada de uma função composta considerando as derivadas das funções componentes.\n",
    "\n",
    "Quando se calcula o gradiente de uma função composta usando a regra da cadeia, o que se obtém é uma expressão para a taxa de variação da função composta em relação a cada uma de suas variáveis. Em outras palavras, o gradiente resultante nos dá a direção e magnitude do maior aumento da função composta no espaço de várias dimensões.\n",
    "\n",
    "Essa informação é extremamente útil em uma série de aplicações, incluindo otimização de funções, onde se deseja encontrar o ponto mínimo ou máximo de uma função, bem como em métodos numéricos e aplicações de Machine Learning, como no treinamento de redes neurais com o método do gradiente descendente.\n",
    "\n",
    "Vamos implementar uma rede neural simples com apenas um neurônio (também chamado de perceptron) para demonstrar isso. Usaremos a biblioteca PyTorch, que lida automaticamente com a regra da cadeia durante a retropropagação.\n",
    "\n",
    "Considere Fórmula Matemática: \n",
    "$$y = x * w$$ \n",
    "\n",
    "**y**: variável de saída\\\n",
    "**x**: variável de entrada\\\n",
    "**w**: o que estabelece esse relacionamento de x e y\n",
    "\n",
    "Para criar nosso exemplo, vamos definir os conceitos abaixo:\n",
    "\n",
    "**Cálculo do Gradiente**: Em redes neurais, o processo de aprendizado envolve otimizar os parâmetros (ou pesos) da rede para minimizar a função de perda (ou erro). Isso é feito usando técnicas de otimização como o gradiente descendente. Para aplicar essas técnicas, é necessário calcular o gradiente da função de perda em relação a cada parâmetro. O gradiente é essencialmente a taxa de mudança da função de perda com respeito a esses parâmetros, ou seja, a derivada.\n",
    "\n",
    "**Backpropagation**: O cálculo do gradiente é realizado através de um processo chamado backpropagation. Para isso, as bibliotecas de aprendizado de máquina mantêm um grafo de computação que registra todas as operações realizadas nos tensores que têm requires_grad definido como True. Quando a função de perda é calculada, o gradiente dessa perda é propagado de volta através do grafo e os gradientes em relação a cada tensor são acumulados.\n",
    "\n",
    "**requires_grad=True**: Ao definir requires_grad=True para um tensor no PyTorch, você está informando à biblioteca que deseja que ela calcule os gradientes desse tensor durante a passagem para trás (backpropagation). Normalmente, isso é feito para os parâmetros da rede que você deseja otimizar. Por exemplo, pesos e vieses em uma rede neural teriam requires_grad=True.\n",
    "\n",
    "**Otimização e Atualização de Parâmetros**: Durante o treinamento, esses gradientes são usados por otimizadores (como SGD, Adam, etc.) para atualizar os parâmetros da rede na direção que minimiza a função de perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6467bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43243c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de entrada x\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9000b145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300, requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de peso w (coeficiente do modelo, o que o modelo vai aprender no treinamento )\n",
    "w = torch.tensor(0.03, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e912153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializa o tensor de saída y (previsão do modelo)\n",
    "y = torch.tensor(1.0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e82208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função de ativação como função de identidade (f(x) = x)\n",
    "funcao_ativacao = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c55c7b",
   "metadata": {},
   "source": [
    "Uma das principais razões para usar funções de ativação é introduzir não linearidade no modelo. Redes neurais são projetadas para aproximar funções complexas e a maioria dos problemas do mundo real que queremos modelar são não lineares por natureza. Sem funções de ativação não lineares, uma rede neural, independentemente de sua profundidade (número de camadas), seria equivalente a um modelo linear e, portanto, incapaz de modelar a complexidade encontrada em tarefas reais como reconhecimento de imagem, processamento de linguagem natural, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec70728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0300, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcula a saída da rede neural\n",
    "y_previsto = funcao_ativacao(w * x)\n",
    "y_previsto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e21df",
   "metadata": {},
   "source": [
    "Calcular o erro quadrado médio (MSE, do inglês Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cb27dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9409, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao_de_erro = (y_previsto - y)**2\n",
    "funcao_de_erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fcfe8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.7600)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao_de_erro = torch.nn.MSELoss()\n",
    "erro = funcao_de_erro(y_previsto, y)\n",
    "# Usa a retropropagação para calcular o gradiente do erro em relação a w\n",
    "erro.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2511f",
   "metadata": {},
   "source": [
    "O script Python acima define um neurônio com uma entrada (x) e um peso (w), e usa a regra da cadeia para calcular o gradiente da função de perda com respeito ao peso. O valor de gradiente resultante pode ser usado para atualizar o peso e treinar a rede neural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82177e9",
   "metadata": {},
   "source": [
    "## Compreendendo o Algoritmo da Descida do Gradiente\n",
    "\n",
    "O algoritmo da descida do gradiente é um método de otimização utilizado principalmente para encontrar o ponto mínimo de uma função. Ele é amplamente utilizado em aprendizado de máquina e inteligência artificial, especialmente no treinamento de modelos de redes neurais. Vamos entender como ele funciona.\n",
    "\n",
    "**Objetivo**: O objetivo principal da descida do gradiente é minimizar uma função de custo ou erro. Essa função mede a diferença entre a saída prevista pelo modelo e a saída real dos dados. Por exemplo, em um modelo de regressão linear, a função de custo pode ser o erro quadrático médio entre as previsões do modelo e os valores reais.\n",
    "\n",
    "**Gradiente**: O gradiente é o vetor de derivadas parciais da função de custo. Ele aponta na direção do maior aumento da função. Intuitivamente, você pode pensar no gradiente como uma bússola que indica em qual direção você deve ir para mudar o valor da função o mais rápido possível.\n",
    "\n",
    "**Passo Inverso**: Na descida do gradiente, em vez de seguir na direção do aumento da função, vamos na direção oposta, ou seja, seguimos o gradiente negativo. Isso é feito porque queremos minimizar a função de custo, não maximizá-la.\n",
    "\n",
    "**Taxa de Aprendizado**: Este é um parâmetro essencial no algoritmo da descida do gradiente. A taxa de aprendizado define o tamanho dos passos que você dá na direção oposta ao gradiente. Se for muito pequena, o algoritmo demora muito para convergir para o mínimo. Se for muito grande, pode ultrapassar o mínimo ou até divergir.\n",
    "\n",
    "**Atualização Iterativa:** O processo é iterativo. Em cada etapa, calculamos o gradiente da função de custo com base nos parâmetros atuais, e então atualizamos esses parâmetros subtraindo o produto da taxa de aprendizado pelo gradiente. Esse processo se repete até que o algoritmo converge, o que significa que os parâmetros não mudam significativamente, ou até que um número máximo de iterações seja alcançado.\n",
    "\n",
    "**Variações**: Existem diversas variações da descida do gradiente, como a descida do gradiente estocástico (SGD) e o gradiente descendente com momentum. Essas variações tentam melhorar a eficiência e a eficácia do algoritmo em diferentes cenários, como evitar mínimos locais ou acelerar a convergência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109cccc",
   "metadata": {},
   "source": [
    "### Gradiente Descendente via Operações Matemáticas com Linguagem Python\n",
    "\n",
    "Uma aplicação comum de derivadas em Data Science é na implementação do algoritmo de gradiente descendente, usado para otimizar funções de custo em modelos de aprendizado de máquina, como regressão linear ou redes neurais.\n",
    "\n",
    "Aqui está um exemplo de como isso pode ser feito em Python para a tarefa de regressão linear. Para simplificar, vamos considerar uma regressão linear simples com apenas uma variável de entrada.\n",
    "\n",
    "No exemplo abaixo, o código cria um conjunto de dados aleatório, inicializa os parâmetros da regressão linear de forma aleatória (a variável theta), e então executa o algoritmo de gradiente descendente por um número fixo de iterações.\n",
    "\n",
    "A cada iteração, o algoritmo calcula o gradiente da função de custo em relação aos parâmetros (gradientes) e, em seguida, atualiza os parâmetros na direção oposta ao gradiente (isso é o que a linha theta = theta - lr * gradientes faz). O tamanho do passo é determinado pela taxa de aprendizado (lr).\n",
    "\n",
    "Ao final do processo, os parâmetros da regressão linear (ou seja, a inclinação e o intercepto) são armazenados na variável theta. Esses parâmetros minimizam a função de custo e, portanto, representam a melhor linha de ajuste aos dados.\n",
    "\n",
    "Considere que X é o diâmetro de uma Pizza que um cliente pediu e y a gorjeta dada por um cliente. Conseguimos prever a gorjeta com base no diâmetro da Pizza?\n",
    "\n",
    "Vamos definir uma relação linear entre X e y:\n",
    "\n",
    "y = coef1 + (coef2 * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e1a0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cpnjunto de dados sintéticos\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "311d92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1  # Taxa de aprendizado (controla a velocidade de mudança dos pesos)\n",
    "n_interations = 1000  # Número de iterações para o treinamento\n",
    "m = 100  # Número de amostras no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c085067c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6740945 ],\n",
       "       [0.52394803]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.random.randn(2,1)  # Inicializa os parâmetros (pesos) aleatoriamente\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a058d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.03212476],\n",
       "       [1.        , 1.47162265],\n",
       "       [1.        , 0.69924422],\n",
       "       [1.        , 1.77730675],\n",
       "       [1.        , 0.06804411]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adiciona a coluna de 1s a X para o termo de interceptação\n",
    "X_b = np.c_[np.ones((m, 1)), X] \n",
    "X_b[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ceffd2",
   "metadata": {},
   "source": [
    "A função de custo (erro quadrático médio) é definida como:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\big( \\hat{y}^{(i)} - y^{(i)} \\big)^2\n",
    "$$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $m$ = número de exemplos de treino  \n",
    "- $\\hat{y}^{(i)} = X^{(i)}\\theta$ = predição do modelo  \n",
    "- $y^{(i)}$ = valor real  \n",
    "\n",
    "Na forma vetorial:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} (X\\theta - y)^T (X\\theta - y)\n",
    "$$\n",
    "\n",
    "\n",
    "Expandindo a multiplicação:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\Big[ \\theta^T X^T X \\theta - 2y^T X \\theta + y^T y \\Big]\n",
    "$$\n",
    "\n",
    "Derivada em relação a $\\theta$\n",
    "\n",
    "Usamos as seguintes regras de derivada matricial:\n",
    "\n",
    "1. $$\\frac{\\partial}{\\partial \\theta} \\big( \\theta^T A \\theta \\big) = (A + A^T)\\theta$$  \n",
    "   Se $A$ é simétrica: $$= 2A\\theta$$  \n",
    "\n",
    "2. $$\\frac{\\partial}{\\partial \\theta} (b^T \\theta) = b$$  \n",
    "\n",
    "3. $$\\frac{\\partial}{\\partial \\theta} (c) = 0$$  \n",
    "\n",
    "Aplicando essas regras:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} \\Big( 2X^T X \\theta - 2X^T y \\Big)\n",
    "$$\n",
    "\n",
    "Simplificação:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{m} X^T (X\\theta - y)\n",
    "$$\n",
    "\n",
    "Interpretação:\n",
    "\n",
    "- $(X\\theta - y)$ → vetor dos **erros (resíduos)**  \n",
    "- $X^T (X\\theta - y)$ → projeta os erros de volta nas direções das variáveis de entrada  \n",
    "- $\\tfrac{2}{m}$ → normalização  \n",
    "\n",
    "Atualização dos parâmetros\n",
    "\n",
    "No **gradiente descendente**, atualizamos:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "onde $\\eta$ é a taxa de aprendizado (*learning rate*).  \n",
    "\n",
    "\n",
    "Resultado final:\n",
    "\n",
    "$$\n",
    "\\boxed{\\nabla_\\theta J(\\theta) = \\frac{2}{m} X^T (X\\theta - y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440152be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for interation in range(n_interations):\n",
    "  \n",
    "   # Calcula o gradiente (derivada do erro em relação aos parâmetros)\n",
    "   gradiente = 2/m * X_b.T.dot( X_b.dot(theta) - y)\n",
    "\n",
    "   # Atualiza os parâmetros\n",
    "   theta = theta - lr * gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ce8ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.55271368e-15]\n",
      " [ 2.01616501e-15]]\n"
     ]
    }
   ],
   "source": [
    "print(gradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea1d1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.01120076]\n",
      " [3.0594992 ]]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1528e",
   "metadata": {},
   "source": [
    "---\n",
    "Materiais Sobre Derivada:\n",
    "\n",
    "https://estudoemcasaapoia.dge.mec.pt/recurso/nocao-de-derivada-num-ponto \n",
    "\n",
    "https://br.neurochispas.com/calculo/10-exercicios-de-derivadas-usando-limites/\n",
    "\n",
    "https://embuscadosaber.com/derivada-de-uma-funcao-usando-definicao/\n",
    "\n",
    "https://www.infoescola.com/matematica/calculo-de-derivadas/\n",
    "\n",
    "https://brasilescola.uol.com.br/matematica/introducao-ao-estudo-das-derivadas.htm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

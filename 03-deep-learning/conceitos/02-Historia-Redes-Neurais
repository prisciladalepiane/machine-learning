# Uma breve história da evolução das redes neurais

## 1958
Frank Rosenblatt introduz o "_Perceptron_", a primeira arquitetura de aprendizado de máquina. O percepton era uma rede neural de camada única, capaz de realizar tarefas de classificação linear.

## 1969
Marvin Minsky e Seymour Pepert publicam o livro "_Perceptrons_", 
que destaca limitações significativas do perceptron, especialmente sua incapacidade de resolver problemas não lineares. Isso levou a redução no intusiasmo e no finaciamento para pesquisas em redes neurais. 

## 1986
David Rumelhart, Geoffrey Hinton e Ronal Williams introduzem o algoritmo de _**backpropagation**_ (usado até hoje). Esse método permite treinar **redes neurais multicamadas**, superando algumas das limitações destacadas por Minsky e Pepert.

## 1989
Elman introduz as **Redes Neurais Recorrentes (RNNs)** adequadas para processar sequência de dados. RNNs causaram uma revolução em Processamento de Linguagem Natural.

## 1997
Long Short-Term Memory (LSTM) é proposta por Hochreiter e Schmidhuber, uma variante de RNN projetada para capturar dependências de longo prazo em sequências de dados.

## 2012
AlexNet, uma rede Neural Concolucional profunda, vence a competição ImageNet, estabelecendo Deep Learning como uma técnica dominante em visão computacional. A disponibilidade de grandes conjuntos de dados e o poder de computação proporcionado por GPUs tornam o treinamento de redes profundas viável.

## 2017
É puplicado o paper de pesquisa chamado _"Attention is All You Need"_ estabelecendo a arquitetura **Transformer**, que revolucionaria novamente o campo do Processamento de Linguagem Natural e Deep Learning como um todo. As técnicas mais avançadas nos dias de hoje usam arquitetura Transformer e suas variações.

## 2020
Surgiram modelos de Grande Escala (Large Models - **LLMs**), como GPT-3 da OpenAI, apresentando bilhões de parâmetros, demonstrando capacidades impressionantes de geração de linguagem.

## 2022
Uma interface web para o modelo GPT-3 é disponibilizado ao público gratuitamente e isso cria uma explosão no interesse em IA. O ChatGPT.

## Dias de hoje 
Explosão no uso de LLMs através de IA Generativa.






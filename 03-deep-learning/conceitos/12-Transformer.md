# Arquitetura Transformer

A Arquitetura Transformer, introduzida no artigo "Attention is Allt You Need" de Vaswani"
é uma arquitetura que depende exclusivamente de mecanismos de atenção, sem recorrência (como em ENNs) ou convoluções (como em CNNs).

Mecanismos de atenção e transformadores são conceitos avançados em Inteligência Artificial que têm revolucionado particularmente o campo do Processamento de Linguagem Natural (PLN), e também têm sido aplicados com sucesso em outras áreas , como visão computacional e Séries Temporais.

O mecanismo de atenção permite que os modelos de aprendizado de máquina foquem em partes específicas da entrada quando estão processando dados, de forma similar à atenção seletiva dos humanos.

Em vez de tratar todas as partes de entrada igualmente, o mecanismo de atenção aprende a distribuir pesos diferentes indicando quais partes são mais relevantes para a tarefa em questão.

A arquiterura transformer foi criada inicialmente para problemas de PLN mas pode ser empregada em diversas áreas. Porém transformers requer uma quantidade massiva de dados (bilhões/trilhões).


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eefb5d6",
   "metadata": {},
   "source": [
    "# Implementando o Mecanismo de Atenção em Python sem uso de frameworks\n",
    "\n",
    "Esse projeto tem como objetivo aprender como funciona o mecanismo de atenção em um modelo Transformer. \n",
    "\n",
    "Attention Is All You Need: https://arxiv.org/abs/1706.03762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec49435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3549d",
   "metadata": {},
   "source": [
    "### Classe que implementa um modelo Transformer para aprendizado de sequência usando PyTorch\n",
    "\n",
    "`nn.Module` (nn = neural network) é a classe base fundamental de todos os modelos e camadas do PyTorch. Quase tudo que constrído no PyTorch herda dela: redes neurais completas, camadas individuais, blocos de atenção, embeddings, etc.\n",
    "\n",
    "Camadas:\n",
    "\n",
    "`nn.Embedding` é uma camada do PyTorch que transforma cada token representado por um número inteiro em um vetor denso de dimensão fixa. Ela funciona como uma tabela de lookup onde cada índice do vocabulário corresponde a um vetor treinável, permitindo que a rede aprenda representações contínuas e significativas para palavras ou tokens. Em vez de usar one-hot vectors enormes e esparsos, o embedding fornece vetores compactos que capturam relações semânticas e sintáticas durante o treinamento, servindo como a primeira etapa essencial em modelos de NLP como Transformers, RNNs e modelos de linguagem.\n",
    "\n",
    "`nn.MultiheadAttention`: é uma camada do PyTorch que implementa o mecanismo de atenção multi-cabeça usado nos Transformers. Ela recebe vetores de entrada e aprende a calcular relações entre eles usando várias “cabeças” de atenção paralelas, onde cada cabeça foca em um tipo diferente de dependência entre tokens. Essa camada combina consultas, chaves e valores (Q, K, V), aplica atenção escalonada e depois concatena os resultados das cabeças. É uma camada completa, treinável e central para a arquitetura Transformer.\n",
    "\n",
    "`nn.Sequential` com Linear → ReLU → Linear: isso também é uma camada, mais especificamente um bloco feed-forward totalmente conectado, típico de cada camada do Transformer. O nn.Sequential apenas organiza várias camadas em sequência: a primeira nn.Linear projeta o vetor para uma dimensão intermediária (às vezes maior), o nn.ReLU introduz não linearidade e a segunda nn.Linear traz de volta para a dimensão original. Esse bloco é aplicado posição\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202c94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, dropout):\n",
    "\n",
    "        # Inicializa a classe base nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Camada de embedding para converter tokens em vetores densos\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Define o mecanismo de atenção multi-cabeça\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, n_heads, dropout=dropout)\n",
    "\n",
    "        # Camada feed-forward para processamento adicional\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size) \n",
    "\n",
    "        # DEfine a camada de saída final que transformará a seuqencia de saída \n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Aplica a camada de embedding\n",
    "        x = self.embedding(x)   \n",
    "\n",
    "        # Aplica o mecanismo de atenção multi-cabeça\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Aplica a camada feed-forward\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # Aplica a camada de saída final\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f643ef",
   "metadata": {},
   "source": [
    "Parâmetros: \n",
    "\n",
    "\n",
    "`b_size`: representa o número total de tokens que o modelo consegue reconhecer e gerar. Ele define o tamanho da tabela de embeddings e o número de classes no softmax final. Um vocabulário maior permite representar mais palavras e subpalavras, mas aumenta a memória e o custo computacional. Em resumo, o vocab_size determina o universo linguístico em que o modelo opera.\n",
    "\n",
    "`embedding_dim`: é a dimensão dos vetores usados para representar cada token do vocabulário. Cada palavra é convertida em um vetor contínuo desse tamanho, que carrega informações semânticas e sintáticas. Dimensões maiores aumentam a capacidade expressiva do modelo, mas também exigem mais memória e computação. É um dos hiperparâmetros que mais influenciam a qualidade do modelo.\n",
    "\n",
    "`n_heads`: é o número de cabeças de atenção utilizadas no mecanismo de Multi-Head Attention. Cada cabeça aprende a focar em padrões diferentes da sequência, como relações sintáticas ou dependências distantes. Ao usar várias cabeças em paralelo, o modelo capta múltiplos tipos de interação entre tokens ao mesmo tempo. Mais cabeças aumentam a capacidade, mas exigem que embedding_dim seja divisível pelo número de cabeças.\n",
    "\n",
    "`n_layers`: indica quantas camadas empilhadas formam o modelo Transformer. Cada camada contém atenção multi-head, normalização e redes feed-forward, expandindo a profundidade e a capacidade de abstração. Mais camadas permitem que o modelo aprenda padrões mais complexos, porém aumentam tempo de treinamento e risco de overfitting. Esse parâmetro define o “tamanho” estrutural do modelo.\n",
    "\n",
    "`dropout`: é a taxa de probabilidade usada para desligar aleatoriamente partes da rede durante o treinamento. Isso força o modelo a não depender de unidades específicas e melhora a generalização. Valores típicos variam entre 0.1 e 0.3, equilibrando regularização e desempenho. O dropout reduz overfitting e torna o modelo mais robusto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b626dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Transformer(vocab_size = 1000, \n",
    "                     embedding_dim = 32, \n",
    "                     n_heads = 4, \n",
    "                     n_layers = 2, \n",
    "                     dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59706557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.attention    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5eaa6d",
   "metadata": {},
   "source": [
    "## Construindo o modelo transformer sem uso de framework\n",
    "\n",
    "principais camadas do modelo transformer:\n",
    "\n",
    "1 - Camada de Embedding: transforma as palavras em vetores numéricos de tamanho fixo.\n",
    "\n",
    "2 - Mecanismo de Atenção: permite que o modelo foque em diferentes partes de entrada.\n",
    "\n",
    "3 - Camadas encoder e decoder: processam dados sequencialmente.\n",
    "\n",
    "4 - Camada Linear e Sofmax: para predições finais.\n",
    "\n",
    "### Hiperparâmetros Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9a26885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3b8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimenão do modelo\n",
    "dim_model = 64\n",
    "\n",
    "# Cria um vetor de exemplo\n",
    "x = np.random.rand(10, dim_model)  # Sequência de 10 tokens\n",
    "\n",
    "# Tamanho do vocabulário\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a7a58",
   "metadata": {},
   "source": [
    "### Camada Embedding\n",
    "\n",
    "A função _embedding_ é utilizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecidos como embeddings e são uma parte fundamental, em especial dos modelos de PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31b60310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para criar uma matriz de embedding\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "    \n",
    "    # Cria uma matriz de embedding onde cada linha representa um token do vocabulário\n",
    "    # A matriz é inicializada com valores aleatórios normalmente distribuídos\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "    \n",
    "    # Para cada índice de token no input, seleciona o embedding correspondente da matriz\n",
    "    # Retorna um array de embeddings correspondentes à sequência de entrada\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8ce91",
   "metadata": {},
   "source": [
    "### Mecanismos de Atenção\n",
    "\n",
    "O mecanismo de atenção é a parte do Transformer que decide quais palavras da frase são importantes para entender cada palavra atual.\n",
    "\n",
    "No Transformer, Q, K e V são derivados da mesma entrada em camadas de atenção do encoder, mas de entradas diferentes no decoder (Q vem da saída da camada anterior do decoder, enquanto K e V vêm da saída do encoder). O mecanismo de atenção calcula um conjunto de pontuações (usando o produto escalar entre Q e K, daí o nome \"scaled dot-product attention\"), aplica uma função softmax para obter pesos de atenção e usa esses pesos para ponderar os values, criando uma saída que é uma combinação ponderada das informações relevantes de entrada.\n",
    "\n",
    "Este processo permite que o modelo dê \"atenção\" às partes mais relevantes da entrada para cada parte da saída, o que é especialmente útil em tarefas como tradução, onde a relevância de diferentes palavras da entrada pode variar dependendo da parte da frase que está sendo traduzida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add24dd4",
   "metadata": {},
   "source": [
    "### Função de Ativação Softmax\n",
    "\n",
    "A função softmax é uma função de ativação amplamente utilizada em redes neurais, especialmente em cenários de classificação, onde é importante transformar valores brutos de saída (logits) em probabilidades que somam 1. Abaixo, está o código da função softmax com comentários em cada linha explicando seu funcionamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca6bc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação softmax\n",
    "def softmax(x):\n",
    "    \n",
    "    # Calcula o exponencial de cada elemento do input, ajustado pelo máximo valor no input \n",
    "    # para evitar overflow numérico\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    \n",
    "    # Divide cada exponencial pelo somatório dos exponenciais ao longo do último eixo (axis=-1)\n",
    "    # O reshape(-1, 1) garante que a divisão seja realizada corretamente em um contexto multidimensional\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982160d4",
   "metadata": {},
   "source": [
    "### Scale Dot Product\n",
    "\n",
    "A função scaled_dot_product_attention() é um componente do mecanismo de atenção em modelos Transformer. Ela calcula a atenção entre conjuntos de queries (Q), keys (K) e values (V). \n",
    "\n",
    "Essencialmente, essa função permite que o modelo dê importância diferenciada a diferentes partes da entrada, um aspecto chave que torna os modelos Transformer particularmente eficazes para tarefas de PLN e outras tarefas sequenciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e193a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para calcular a atenção escalada por produto escalar\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \n",
    "    # Calcula o produto escalar entre Q e a transposta de K\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "    \n",
    "    # Obtém a dimensão dos vetores de chave\n",
    "    depth = K.shape[-1]\n",
    "    \n",
    "    # Escala os logits (resultado) dividindo-os pela raiz quadrada da profundidade\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "    \n",
    "    # Aplica a função softmax para obter os pesos de atenção\n",
    "    attention_weights = softmax(logits)\n",
    "    \n",
    "    # Multiplica os pesos de atenção pelos valores V para obter a saída final\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    # Retorna a saída ponderada\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe50c1",
   "metadata": {},
   "source": [
    "### Saída do Modelo com Operação Linear e Softmax\n",
    "\n",
    "A função linear_and_softmax() é uma combinação de uma camada linear seguida por uma função softmax, comumente usada em modelos de aprendizado profundo, especialmente em tarefas de classificação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34c72af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função que aplica uma transformação linear seguida de softmax\n",
    "def linear_and_softmax(input):\n",
    "    \n",
    "    # Inicializa uma matriz de pesos com valores aleatórios normalmente distribuídos\n",
    "    # Esta matriz conecta cada dimensão do modelo (dim_model) a cada palavra do vocabulário (vocab_size)\n",
    "    weights = np.random.randn(dim_model, vocab_size)\n",
    "    \n",
    "    # Realiza a operação linear (produto escalar) entre a entrada e a matriz de pesos\n",
    "    # O resultado, logits, é um vetor que representa a entrada transformada em um espaço de maior dimensão\n",
    "    logits = np.dot(input, weights)\n",
    "    \n",
    "    # Aplica a função softmax aos logits\n",
    "    # Isso transforma os logits em um vetor de probabilidades, onde cada elemento soma 1\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5a945",
   "metadata": {},
   "source": [
    "### Construindo o Modelo Final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03d0d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função do modelo final\n",
    "def transformer_model(input):\n",
    "    \n",
    "    # Embedding\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "\n",
    "    # Mecanismo de Atenção \n",
    "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
    "    \n",
    "    # Camada linear e softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "\n",
    "    # Escolhendo os índices com maior probabilidade\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    \n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c52c45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 64\n",
    "seq_length = 10\n",
    "vocab_size = 100\n",
    "\n",
    "# Gerando dados aleatórios para a entrada do modelo\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1801e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência de entrada: [45 34 84 90 19 73 14 49  8 47]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequência de entrada:\", input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9bdb464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência de saída (índices do token previsto pelo modelo):\n",
      " [71 58 18 86 99 72 47 15 94 57]\n"
     ]
    }
   ],
   "source": [
    "output_seq = transformer_model(input_sequence)\n",
    "\n",
    "print(\"Sequência de saída (índices do token previsto pelo modelo):\\n\",\n",
    "       output_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25a0d3",
   "metadata": {},
   "source": [
    "Esse é uma passada, ou seja, um bloco do modelo, para que o modelo tenha boa performace, precisa que tenham várias passadas para que o modelo aprenda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eefb5d6",
   "metadata": {},
   "source": [
    "# Implementando o Mecanismo de Atenção em Python sem uso de frameworks\n",
    "\n",
    "Esse projeto tem como objetivo aprender como funciona o mecanismo de atenção em um modelo Transformer. \n",
    "\n",
    "Attention Is All You Need: https://arxiv.org/abs/1706.03762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec49435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3549d",
   "metadata": {},
   "source": [
    "### Classe que implementa um modelo Transformer para aprendizado de sequência usando PyTorch\n",
    "\n",
    "`nn.Module` (nn = neural network) é a classe base fundamental de todos os modelos e camadas do PyTorch. Quase tudo que constrído no PyTorch herda dela: redes neurais completas, camadas individuais, blocos de atenção, embeddings, etc.\n",
    "\n",
    "Camadas:\n",
    "\n",
    "`nn.Embedding` é uma camada do PyTorch que transforma cada token representado por um número inteiro em um vetor denso de dimensão fixa. Ela funciona como uma tabela de lookup onde cada índice do vocabulário corresponde a um vetor treinável, permitindo que a rede aprenda representações contínuas e significativas para palavras ou tokens. Em vez de usar one-hot vectors enormes e esparsos, o embedding fornece vetores compactos que capturam relações semânticas e sintáticas durante o treinamento, servindo como a primeira etapa essencial em modelos de NLP como Transformers, RNNs e modelos de linguagem.\n",
    "\n",
    "`nn.MultiheadAttention`: é uma camada do PyTorch que implementa o mecanismo de atenção multi-cabeça usado nos Transformers. Ela recebe vetores de entrada e aprende a calcular relações entre eles usando várias “cabeças” de atenção paralelas, onde cada cabeça foca em um tipo diferente de dependência entre tokens. Essa camada combina consultas, chaves e valores (Q, K, V), aplica atenção escalonada e depois concatena os resultados das cabeças. É uma camada completa, treinável e central para a arquitetura Transformer.\n",
    "\n",
    "`nn.Sequential` com Linear → ReLU → Linear: isso também é uma camada, mais especificamente um bloco feed-forward totalmente conectado, típico de cada camada do Transformer. O nn.Sequential apenas organiza várias camadas em sequência: a primeira nn.Linear projeta o vetor para uma dimensão intermediária (às vezes maior), o nn.ReLU introduz não linearidade e a segunda nn.Linear traz de volta para a dimensão original. Esse bloco é aplicado posição\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202c94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, dropout):\n",
    "\n",
    "        # Inicializa a classe base nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Camada de embedding para converter tokens em vetores densos\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Define o mecanismo de atenção multi-cabeça\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, n_heads, dropout=dropout)\n",
    "\n",
    "        # Camada feed-forward para processamento adicional\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size) \n",
    "\n",
    "        # DEfine a camada de saída final que transformará a seuqencia de saída \n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Aplica a camada de embedding\n",
    "        x = self.embedding(x)   \n",
    "\n",
    "        # Aplica o mecanismo de atenção multi-cabeça\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Aplica a camada feed-forward\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # Aplica a camada de saída final\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f643ef",
   "metadata": {},
   "source": [
    "Parâmetros: \n",
    "\n",
    "\n",
    "`b_size`: representa o número total de tokens que o modelo consegue reconhecer e gerar. Ele define o tamanho da tabela de embeddings e o número de classes no softmax final. Um vocabulário maior permite representar mais palavras e subpalavras, mas aumenta a memória e o custo computacional. Em resumo, o vocab_size determina o universo linguístico em que o modelo opera.\n",
    "\n",
    "`embedding_dim`: é a dimensão dos vetores usados para representar cada token do vocabulário. Cada palavra é convertida em um vetor contínuo desse tamanho, que carrega informações semânticas e sintáticas. Dimensões maiores aumentam a capacidade expressiva do modelo, mas também exigem mais memória e computação. É um dos hiperparâmetros que mais influenciam a qualidade do modelo.\n",
    "\n",
    "`n_heads`: é o número de cabeças de atenção utilizadas no mecanismo de Multi-Head Attention. Cada cabeça aprende a focar em padrões diferentes da sequência, como relações sintáticas ou dependências distantes. Ao usar várias cabeças em paralelo, o modelo capta múltiplos tipos de interação entre tokens ao mesmo tempo. Mais cabeças aumentam a capacidade, mas exigem que embedding_dim seja divisível pelo número de cabeças.\n",
    "\n",
    "`n_layers`: indica quantas camadas empilhadas formam o modelo Transformer. Cada camada contém atenção multi-head, normalização e redes feed-forward, expandindo a profundidade e a capacidade de abstração. Mais camadas permitem que o modelo aprenda padrões mais complexos, porém aumentam tempo de treinamento e risco de overfitting. Esse parâmetro define o “tamanho” estrutural do modelo.\n",
    "\n",
    "`dropout`: é a taxa de probabilidade usada para desligar aleatoriamente partes da rede durante o treinamento. Isso força o modelo a não depender de unidades específicas e melhora a generalização. Valores típicos variam entre 0.1 e 0.3, equilibrando regularização e desempenho. O dropout reduz overfitting e torna o modelo mais robusto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b626dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Transformer(vocab_size = 1000, \n",
    "                     embedding_dim = 32, \n",
    "                     n_heads = 4, \n",
    "                     n_layers = 2, \n",
    "                     dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59706557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.attention    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5eaa6d",
   "metadata": {},
   "source": [
    "## Construindo o modelo transformer sem uso de framework\n",
    "\n",
    "principais camadas do modelo transformer:\n",
    "\n",
    "1 - Camada de Embedding: transforma as palavras em vetores numéricos de tamanho fixo.\n",
    "\n",
    "2 - Mecanismo de Atenção: permite que o modelo foque em diferentes partes de entrada.\n",
    "\n",
    "3 - Camadas encoder e decoder: processam dados sequencialmente.\n",
    "\n",
    "4 - Camada Linear e Sofmax: para predições finais.\n",
    "\n",
    "### Hiperparâmetros Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9a26885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3b8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimenão do modelo\n",
    "dim_model = 64\n",
    "\n",
    "# Cria um vetor de exemplo\n",
    "x = np.random.rand(10, dim_model)  # Sequência de 10 tokens\n",
    "\n",
    "# Tamanho do vocabulário\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a7a58",
   "metadata": {},
   "source": [
    "### Camada Embedding\n",
    "\n",
    "A função _embedding_ é utilizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecidos como embeddings e são uma parte fundamental, em especial dos modelos de PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b60310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para criar uma matriz de embedding\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "    \n",
    "    # Cria uma matriz de embedding onde cada linha representa um token do vocabulário\n",
    "    # A matriz é inicializada com valores aleatórios normalmente distribuídos\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "    \n",
    "    # Para cada índice de token no input, seleciona o embedding correspondente da matriz\n",
    "    # Retorna um array de embeddings correspondentes à sequência de entrada\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8ce91",
   "metadata": {},
   "source": [
    "### Mecanismos de Atenção"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

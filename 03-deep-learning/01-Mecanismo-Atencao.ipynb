{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eefb5d6",
   "metadata": {},
   "source": [
    "# Implementando o Mecanismo de Atenção em Python sem uso de frameworks\n",
    "\n",
    "Esse projeto tem como objetivo aprender como funciona o mecanismo de atenção em um modelo Transformer. \n",
    "\n",
    "Attention Is All You Need: https://arxiv.org/abs/1706.03762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec49435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3549d",
   "metadata": {},
   "source": [
    "### Classe que implementa um modelo Transformer para aprendizado de sequência usando PyTorch\n",
    "\n",
    "`nn.Module` (nn = neural network) é a classe base fundamental de todos os modelos e camadas do PyTorch. Quase tudo que constrído no PyTorch herda dela: redes neurais completas, camadas individuais, blocos de atenção, embeddings, etc.\n",
    "\n",
    "Camadas:\n",
    "\n",
    "`nn.Embedding` é uma camada do PyTorch que transforma cada token representado por um número inteiro em um vetor denso de dimensão fixa. Ela funciona como uma tabela de lookup onde cada índice do vocabulário corresponde a um vetor treinável, permitindo que a rede aprenda representações contínuas e significativas para palavras ou tokens. Em vez de usar one-hot vectors enormes e esparsos, o embedding fornece vetores compactos que capturam relações semânticas e sintáticas durante o treinamento, servindo como a primeira etapa essencial em modelos de NLP como Transformers, RNNs e modelos de linguagem.\n",
    "\n",
    "`nn.MultiheadAttention`: é uma camada do PyTorch que implementa o mecanismo de atenção multi-cabeça usado nos Transformers. Ela recebe vetores de entrada e aprende a calcular relações entre eles usando várias “cabeças” de atenção paralelas, onde cada cabeça foca em um tipo diferente de dependência entre tokens. Essa camada combina consultas, chaves e valores (Q, K, V), aplica atenção escalonada e depois concatena os resultados das cabeças. É uma camada completa, treinável e central para a arquitetura Transformer.\n",
    "\n",
    "`nn.Sequential` com Linear → ReLU → Linear: isso também é uma camada, mais especificamente um bloco feed-forward totalmente conectado, típico de cada camada do Transformer. O nn.Sequential apenas organiza várias camadas em sequência: a primeira nn.Linear projeta o vetor para uma dimensão intermediária (às vezes maior), o nn.ReLU introduz não linearidade e a segunda nn.Linear traz de volta para a dimensão original. Esse bloco é aplicado posição\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202c94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, dropout):\n",
    "\n",
    "        # Inicializa a classe base nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Camada de embedding para converter tokens em vetores densos\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Define o mecanismo de atenção multi-cabeça\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, n_heads, dropout=dropout)\n",
    "\n",
    "        # Camada feed-forward para processamento adicional\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size) \n",
    "\n",
    "        # DEfine a camada de saída final que transformará a seuqencia de saída \n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Aplica a camada de embedding\n",
    "        x = self.embedding(x)   \n",
    "\n",
    "        # Aplica o mecanismo de atenção multi-cabeça\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Aplica a camada feed-forward\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # Aplica a camada de saída final\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f643ef",
   "metadata": {},
   "source": [
    "Parâmetros: \n",
    "\n",
    "\n",
    "`b_size`: representa o número total de tokens que o modelo consegue reconhecer e gerar. Ele define o tamanho da tabela de embeddings e o número de classes no softmax final. Um vocabulário maior permite representar mais palavras e subpalavras, mas aumenta a memória e o custo computacional. Em resumo, o vocab_size determina o universo linguístico em que o modelo opera.\n",
    "\n",
    "`embedding_dim`: é a dimensão dos vetores usados para representar cada token do vocabulário. Cada palavra é convertida em um vetor contínuo desse tamanho, que carrega informações semânticas e sintáticas. Dimensões maiores aumentam a capacidade expressiva do modelo, mas também exigem mais memória e computação. É um dos hiperparâmetros que mais influenciam a qualidade do modelo.\n",
    "\n",
    "`n_heads`: é o número de cabeças de atenção utilizadas no mecanismo de Multi-Head Attention. Cada cabeça aprende a focar em padrões diferentes da sequência, como relações sintáticas ou dependências distantes. Ao usar várias cabeças em paralelo, o modelo capta múltiplos tipos de interação entre tokens ao mesmo tempo. Mais cabeças aumentam a capacidade, mas exigem que embedding_dim seja divisível pelo número de cabeças.\n",
    "\n",
    "`n_layers`: indica quantas camadas empilhadas formam o modelo Transformer. Cada camada contém atenção multi-head, normalização e redes feed-forward, expandindo a profundidade e a capacidade de abstração. Mais camadas permitem que o modelo aprenda padrões mais complexos, porém aumentam tempo de treinamento e risco de overfitting. Esse parâmetro define o “tamanho” estrutural do modelo.\n",
    "\n",
    "`dropout`: é a taxa de probabilidade usada para desligar aleatoriamente partes da rede durante o treinamento. Isso força o modelo a não depender de unidades específicas e melhora a generalização. Valores típicos variam entre 0.1 e 0.3, equilibrando regularização e desempenho. O dropout reduz overfitting e torna o modelo mais robusto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b626dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Transformer(vocab_size = 1000, \n",
    "                     embedding_dim = 32, \n",
    "                     n_heads = 4, \n",
    "                     n_layers = 2, \n",
    "                     dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59706557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.attention    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

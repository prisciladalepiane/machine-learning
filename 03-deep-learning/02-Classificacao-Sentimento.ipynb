{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bccce2",
   "metadata": {},
   "source": [
    "# Fine-Tuning de um Modelo Transformer para Classificação de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744ef16",
   "metadata": {},
   "source": [
    "## Bibliotecas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf8f5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q numpy==1.26.2\n",
    "#!pip install -q keras\n",
    "#!pip install -q spacy\n",
    "#!pip install -q tensorflow\n",
    "#!pip install -q keras-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "58f55df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, CallbackList, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba18a8",
   "metadata": {},
   "source": [
    "## Dados\n",
    "\n",
    "Dados extraidos da platoforma hugging face\n",
    "\n",
    "https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65f3259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am feeling completely overwhelmed i have two...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have the feeling she was amused and delighted</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was able to help chai lifeline with your sup...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i already feel like i fucked up though because...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i still love my so and wish the best for him i...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  i am feeling completely overwhelmed i have two...     fear\n",
       "1    i have the feeling she was amused and delighted      joy\n",
       "2  i was able to help chai lifeline with your sup...      joy\n",
       "3  i already feel like i fucked up though because...    anger\n",
       "4  i still love my so and wish the best for him i...  sadness"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino = pd.read_csv('/home/priscila/Downloads/03-DeepL/dados/dados_treino_p2.txt', header= None, delimiter=';')\n",
    "dados_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3cd872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel like my only role now would be to tear ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel just bcoz a fight we get mad to each ot...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like reds and purples are just so rich ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im not sure the feeling of loss will ever go a...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel like ive gotten to know many of you thr...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  i feel like my only role now would be to tear ...  sadness\n",
       "1  i feel just bcoz a fight we get mad to each ot...    anger\n",
       "2  i feel like reds and purples are just so rich ...      joy\n",
       "3  im not sure the feeling of loss will ever go a...  sadness\n",
       "4  i feel like ive gotten to know many of you thr...      joy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste = pd.read_csv('/home/priscila/Downloads/03-DeepL/dados/dados_teste_p2.txt', header= None, delimiter=';')\n",
    "dados_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfe20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = dados_treino.rename(columns={0: 'texto', 1: 'sentimento'})\n",
    "dados_teste = dados_teste.rename(columns={0: 'texto', 1: 'sentimento'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1e0f4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dados_treino.shape)\n",
    "print(dados_teste.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a54820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimento\n",
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino['sentimento'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b880ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimento\n",
       "joy         695\n",
       "sadness     581\n",
       "anger       275\n",
       "fear        224\n",
       "love        159\n",
       "surprise     66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste['sentimento'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90944d04",
   "metadata": {},
   "source": [
    "obs: importante que a saída dos dados de treino e teste sejam a mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846388e",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados de texto com spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78aa346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel\n",
    "#import IPython\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b5ffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install spacy -q\n",
    "#!python -m spacy download en_core_web_md\n",
    "#%pip install -U spacy\n",
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a0c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ce582a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Machine learning is the ability of a machine to improve its performance on a specific task\"\n",
    "doc = nlp(text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b98b8968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine - NOUN - machine - False\n",
      "learning - NOUN - learning - False\n",
      "is - AUX - be - True\n",
      "the - DET - the - True\n",
      "ability - NOUN - ability - False\n",
      "of - ADP - of - True\n",
      "a - DET - a - True\n",
      "machine - NOUN - machine - False\n",
      "to - PART - to - True\n",
      "improve - VERB - improve - False\n",
      "its - PRON - its - True\n",
      "performance - NOUN - performance - False\n",
      "on - ADP - on - True\n",
      "a - DET - a - True\n",
      "specific - ADJ - specific - False\n",
      "task - NOUN - task - False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"-\",token.pos_,\"-\" ,token.lemma_, \"-\", token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73835a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para pré-processamento de texto\n",
    "def pre_process(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca15fcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machine learning ability machine improve performance specific task'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aef0dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino['texto'] = dados_treino['texto'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0fbe140",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_teste['texto'] = dados_teste['texto'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46979812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel completely overwhelmed strategy help feel...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto sentimento\n",
       "0  feel completely overwhelmed strategy help feel...       fear"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7116e316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fb290",
   "metadata": {},
   "source": [
    "A função `pre_process` recebeu um texto bruto e realizou um pré-processamento clássico de NLP usando o spaCy, transformando o texto em uma versão padronizada e mais adequada para análises posteriores. Primeiro, o texto é processado pelo pipeline nlp, que tokeniza e analisa linguisticamente o conteúdo. Em seguida, a função percorre cada token, remove palavras irrelevantes (stopwords), converte cada termo para sua forma base (lematização), padroniza para letras minúsculas e elimina espaços extras. Por fim, os tokens processados são reunidos novamente em uma única string, separada por espaços, resultando em um texto limpo, normalizado e menos ruidoso, apropriado para tarefas como vetorização, classificação ou análise exploratória de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05e66",
   "metadata": {},
   "source": [
    "## Modelo 1 - Arquitetura Fully Connect Neural Network\n",
    "\n",
    "### Passo 1: Vetorização com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4483c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719abb1",
   "metadata": {},
   "source": [
    "Esta linha acima cria uma instância do `TfidfVectorizer` da biblioteca scikit-learn, que é uma ferramenta utilizada para converter uma coleção de documentos brutos em uma matriz de recursos TF-IDF (Term Frequency-Inverse Document Frequency). O TF-IDF é uma técnica estatística usada para quantificar a importância de uma palavra em um conjunto de documentos, comumente utilizada em tarefas de processamento de linguagem natural e recuperação de informações.\n",
    "\n",
    "**Parâmetro max_df=0.95**: Este parâmetro define o limite máximo de frequência de documento para os termos que serão considerados. Aqui, está definido como 0.95, o que significa que palavras que aparecem em mais de 95% dos documentos serão ignoradas. Isso ajuda a eliminar palavras comuns que não contribuem muito para o significado do texto.\n",
    "\n",
    "**Parâmetro min_df=2**: Este parâmetro estabelece a frequência mínima de documento para os termos. Neste caso, termos que aparecem em menos de 2 documentos serão ignorados. Isso ajuda a filtrar termos raros que podem ocorrer apenas em poucas amostras e, portanto, são menos relevantes para a análise geral.\n",
    "\n",
    "**Parâmetro stop_words='english'**: Este parâmetro instrui o vetorizador a remover todas as palavras de parada em inglês da análise. Palavras de parada são palavras comuns (como \"e\", \"o\", \"em\") que geralmente são filtradas em processamento de linguagem natural porque são muito frequentes e não carregam informações significativas para a análise de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49abf5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 112822 stored elements and shape (16000, 5593)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplica a vetorização\n",
    "dados_treino_tfidf = tf_idf.fit_transform(dados_treino['texto'])\n",
    "dados_treino_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec753fc",
   "metadata": {},
   "source": [
    "o objeto tf_idf aprende o vocabulário a partir dos textos de treino. Ele identifica todas as palavras que aparecem, calcula com que frequência cada palavra ocorre em cada documento (TF) e o quão rara ou comum ela é no conjunto inteiro (IDF). Ao mesmo tempo, ele já transforma cada texto em um vetor numérico, onde cada posição representa uma palavra do vocabulário e o valor indica sua importância naquele texto. Esse aprendizado só é feito nos dados de treino para evitar vazamento de informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86250cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 13651 stored elements and shape (2000, 5593)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste_tfidf = tf_idf.transform(dados_teste['texto'])\n",
    "dados_teste_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a579936",
   "metadata": {},
   "source": [
    "o modelo não aprende nada novo. Ele apenas usa o vocabulário e os pesos aprendidos no treino para transformar os textos de teste em vetores numéricos compatíveis. Palavras que não existiam no treino são ignoradas, garantindo que os dados de treino e teste fiquem no mesmo espaço vetorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a5de5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 5593)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treino_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31ea4b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5593)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "632b79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aa', 'abandon', 'abandonment', 'abc', 'abdomen', 'abide', 'ability',\n",
      "       'abit', 'able', 'abroad',\n",
      "       ...\n",
      "       'yuuki', 'zach', 'zealand', 'zen', 'zero', 'zest', 'zombie', 'zone',\n",
      "       'zoom', 'zumba'],\n",
      "      dtype='object', length=5593)\n"
     ]
    }
   ],
   "source": [
    "# Visualizando as features criadas pela vetorização TF-IDF\n",
    "feature_names = tf_idf.get_feature_names_out()\n",
    "\n",
    "df_tfidf = pd.DataFrame(\n",
    "    dados_treino_tfidf[:5].toarray(),  # só os 5 primeiros textos\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "print(df_tfidf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c1a46cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abc</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>abit</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>yuuki</th>\n",
       "      <th>zach</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zest</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5593 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  abandon  abandonment  abc  abdomen  abide  ability  abit      able  \\\n",
       "0  0.0      0.0          0.0  0.0      0.0    0.0      0.0   0.0  0.000000   \n",
       "1  0.0      0.0          0.0  0.0      0.0    0.0      0.0   0.0  0.000000   \n",
       "2  0.0      0.0          0.0  0.0      0.0    0.0      0.0   0.0  0.490603   \n",
       "3  0.0      0.0          0.0  0.0      0.0    0.0      0.0   0.0  0.000000   \n",
       "4  0.0      0.0          0.0  0.0      0.0    0.0      0.0   0.0  0.000000   \n",
       "\n",
       "   abroad  ...  yuuki  zach  zealand  zen  zero  zest  zombie  zone  zoom  \\\n",
       "0     0.0  ...    0.0   0.0      0.0  0.0   0.0   0.0     0.0   0.0   0.0   \n",
       "1     0.0  ...    0.0   0.0      0.0  0.0   0.0   0.0     0.0   0.0   0.0   \n",
       "2     0.0  ...    0.0   0.0      0.0  0.0   0.0   0.0     0.0   0.0   0.0   \n",
       "3     0.0  ...    0.0   0.0      0.0  0.0   0.0   0.0     0.0   0.0   0.0   \n",
       "4     0.0  ...    0.0   0.0      0.0  0.0   0.0   0.0     0.0   0.0   0.0   \n",
       "\n",
       "   zumba  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 5593 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12f3af",
   "metadata": {},
   "source": [
    "Trasformar os dados de entrada (texto) para array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58a09293",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dados_treino_tfidf.toarray()\n",
    "X_test = dados_teste_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23394b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16000, 5593)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94096d",
   "metadata": {},
   "source": [
    "Os dados foram vetorizados\n",
    "\n",
    "### Passo 2 - Preparação dos dados\n",
    "\n",
    "Converter a variavel alvo para representação numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "29f6caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a511ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Faz o fit e transform na variável alvo em treino\n",
    "y_treino_le = encoder.fit_transform(dados_treino['sentimento'])\n",
    "print(np.unique(y_treino_le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "18f77e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "y_teste_le = encoder.transform(dados_teste['sentimento'])\n",
    "print(np.unique(y_teste_le))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de2ef0",
   "metadata": {},
   "source": [
    "Tratar o desbalanceamento de classes dando pesos menores para classes com menos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "02f2c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peso das classes\n",
    "pesos_classes = compute_class_weight('balanced', classes = np.unique(y_treino_le), y = y_treino_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dec6eb",
   "metadata": {},
   "source": [
    "**compute_class_weight**: Esta é uma função do scikit-learn que calcula os pesos para as classes. Esses pesos podem ser usados em modelos de classificação para dar mais importância a classes que são sub-representadas no conjunto de dados.\n",
    "\n",
    "**'balanced'**: Este parâmetro indica que os pesos das classes devem ser calculados de forma que equilibrem o conjunto de dados. Isso é feito inversamente proporcional à frequência das classes no conjunto de dados. Classes mais frequentes recebem um peso menor, enquanto classes menos frequentes recebem um peso maior.\n",
    "\n",
    "**classes = np.unique(y_treino_le)**: Aqui, np.unique(y_treino_le) encontra todas as classes únicas no conjunto de dados de treinamento. O parâmetro classes informa à função compute_class_weight quais são essas classes únicas.\n",
    "\n",
    "**y = y_treino_le**: Este é o vetor de rótulos do conjunto de dados de treinamento. A função usará esses rótulos para calcular a frequência de cada classe.\n",
    "\n",
    "O resultado, armazenado em pesos_classes, é um array onde cada classe tem um peso associado. Esses pesos podem ser usados em modelos de classificação (como uma árvore de decisão, um modelo de regressão logística, SVM, etc.) para compensar o desequilíbrio entre as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f85af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em Dados de Treino e Teste (validação)\n",
    "X_treino, X_val, y_treino, y_val = train_test_split(X_train, \n",
    "                                                    y_treino_le, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 42, \n",
    "                                                    stratify = y_treino_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884ec88",
   "metadata": {},
   "source": [
    "Passo 3 - Construção do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aabe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa um modelo sequencial. Modelos sequenciais são uma pilha linear de camadas.\n",
    "modelo1 = Sequential() \n",
    "\n",
    "# Adiciona a primeira camada densa (fully-connected) ao modelo\n",
    "modelo1.add(Dense(4096, # número de neurônios na camada\n",
    "                        \n",
    "                        # Utiliza a função de ativação SELU (Scaled Exponential Linear Unit)\n",
    "                        activation = 'selu',  \n",
    "                        \n",
    "                        # Inicializa os pesos com a distribuição LeCun normal\n",
    "                        kernel_initializer = 'lecun_normal',  \n",
    "                        \n",
    "                        # Define o formato da entrada com base no número de features do X_treino\n",
    "                        input_shape = (X_treino.shape[1],),  \n",
    "                        \n",
    "                        # Aplica regularização L2 para reduzir o overfitting\n",
    "                        kernel_regularizer = tf.keras.regularizers.l2(0.01)))  \n",
    "\n",
    "# Adiciona a segunda camada densa\n",
    "modelo1.add(Dense(2048, \n",
    "                        activation = 'selu',  \n",
    "                        kernel_initializer = 'lecun_normal',  \n",
    "                        kernel_regularizer = tf.keras.regularizers.l2(0.01)))  \n",
    "\n",
    "# Adiciona a terceira camada densa\n",
    "modelo1.add(Dense(1024, \n",
    "                        activation = 'selu',  \n",
    "                        kernel_initializer = 'lecun_normal',  \n",
    "                        kernel_regularizer = tf.keras.regularizers.l2(0.1)))  \n",
    "\n",
    "# Adiciona a quarta camada densa\n",
    "# Camada com 64 neurônios e ativação SELU\n",
    "modelo1.add(Dense(64, activation = 'selu',))  \n",
    "\n",
    "# Adiciona a camada de saída\n",
    "# Camada de saída com 6 neurônios e ativação softmax para classificação multiclasse\n",
    "modelo1.add(Dense(6, activation = 'softmax'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db597420",
   "metadata": {},
   "source": [
    "Passo 4 - Compilação e sumário do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0b3a5be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "array([1.2351397 , 1.3766993 , 0.49732688, 2.0449898 , 0.5715102 ,\n",
       "       4.6620045 ], dtype=float32)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribui pesos específicos ao vetor de bias da última camada do modelo\n",
    "modelo1.layers[-1].bias.assign(pesos_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "43ca4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila o modelo\n",
    "modelo1.compile(optimizer = 'Adam',  # Define o otimizador como 'Adam'. \n",
    "                      loss = tf.losses.categorical_crossentropy,  \n",
    "                      metrics = ['accuracy'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9c1cf",
   "metadata": {},
   "source": [
    " **Adam** é um algoritmo de otimização que pode ser usado no lugar do procedimento clássico de descida \n",
    "do gradiente estocástica para atualizar os pesos da rede iterativamente com base nos dados de treinamento.\n",
    "Define a função de perda como 'categorical_crossentropy'. É adequada para problemas de classificação \n",
    "multiclasse, onde os rótulos são fornecidos em um formato one-hot encoded.\n",
    "Define a métrica de avaliação do modelo como 'accuracy' (acurácia). \n",
    "A acurácia é uma métrica comum para avaliar o desempenho de modelos de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eeaaf5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">22,913,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,390,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m22,913,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m8,390,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,467,846</span> (127.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,467,846\u001b[0m (127.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,467,846</span> (127.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,467,846\u001b[0m (127.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo1.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
